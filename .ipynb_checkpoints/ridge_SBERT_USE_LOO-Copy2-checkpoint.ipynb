{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-eO1tSqzp8YE"
   },
   "source": [
    "This file contains the implementation of ridge (L2) regression for mapping fMRI embeddings to CLIP and word2vec embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d14zQ3DZq6MK"
   },
   "source": [
    "First setup baseline model (word2vec) (Not yet implemented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uT4mOgANq4sG",
    "outputId": "da6cf362-0555-454c-8199-8c5feabb7ffb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/site-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.11/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests->transformers) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-07 20:58:27.088935: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GahCiBSmdpZc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import scipy.io as sio\n",
    "import nibabel as nib\n",
    "from scipy import signal\n",
    "from pathlib import Path\n",
    "from functools import reduce\n",
    "# from google.colab import drive\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import requests\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z63hCATpLLOt",
    "outputId": "b6eba5ad-53b3-4753-d60a-a774b16e59c8"
   },
   "outputs": [],
   "source": [
    "# drive.mount('/content/drive')\n",
    "fMRI_folder = Path('../doi_10.5061_dryad.gt413__v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v1SWm20pKzuU",
    "outputId": "0b848180-1242-48c4-c82d-06e6e80349b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: 0\n",
      "Subject: 1\n",
      "Subject: 2\n",
      "Subject: 3\n",
      "Subject: 4\n",
      "Subject: 5\n",
      "Subject: 6\n",
      "Subject: 7\n"
     ]
    }
   ],
   "source": [
    "NUM_SUBJS = 8\n",
    "subjects_fmri = [] #stores all 8 subject fmri np arrays\n",
    "\n",
    "# fMRI_folder = Path('./doi_10.5061_dryad.gt413__v1')\n",
    "# assert fMRI_folder.exists(), f\"Foldder: {fMRI_folder} does not exist.\"\n",
    "\n",
    "for subj_id in range(8):\n",
    "    print(\"Subject:\",subj_id)\n",
    "#     fmri_file_name = str(subj_id) + '_masked_2d.npy'\n",
    "#     fmri = np.load(fMRI_folder / fmri_file_name)\n",
    "    fmri_file_name = str(subj_id) + '_smooth_nifti_4d.nii'\n",
    "    fmri = nib.load(fMRI_folder / fmri_file_name)\n",
    "    fmri = np.array(fmri.dataobj)\n",
    "    assert isinstance(fmri, np.ndarray), f\"Imported fmri_scan for subject {subj_id} is not of type numpy.ndarray\"\n",
    "    assert(fmri.ndim) == 4, f\"Imported fmri_scan for subject {subj_id} is not 4 dimensional\"\n",
    "    subjects_fmri.append(fmri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cBZTp5VgVEom",
    "outputId": "dfe79b3d-f7d0-4b42-c69b-6a7635586ba8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53, 60, 50, 1311)\n",
      "(53, 60, 50, 1311)\n",
      "(53, 60, 50, 1311)\n",
      "(53, 60, 50, 1311)\n",
      "(53, 60, 50, 1311)\n",
      "(53, 60, 50, 1311)\n",
      "(53, 60, 50, 1311)\n",
      "(53, 60, 50, 1311)\n"
     ]
    }
   ],
   "source": [
    "for x in subjects_fmri:\n",
    "    print(np.shape(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 60 50\n",
      "51 60 49\n",
      "53 59 47\n",
      "53 59 44\n",
      "51 59 45\n",
      "49 56 51\n",
      "52 60 49\n",
      "52 60 48\n"
     ]
    }
   ],
   "source": [
    "mat_files = sorted(list(fMRI_folder.glob('subject_*.mat')))\n",
    "sub_num = 0\n",
    "matrices = []\n",
    "for file in mat_files:\n",
    "    mat_contents = sio.loadmat(file)\n",
    "    matrix = np.zeros((np.shape(subjects_fmri[sub_num])[3],mat_contents[\"meta\"][0][0][2][0][0],))\n",
    "    fmri = subjects_fmri[int(str(file)[-5])-1]\n",
    "    #print(np.shape(fmri))\n",
    "    #print(mat_contents[\"meta\"])\n",
    "\n",
    "    print(mat_contents[\"meta\"][0][0][3][0][0],mat_contents[\"meta\"][0][0][4][0][0],mat_contents[\"meta\"][0][0][5][0][0])\n",
    "    for x in range(mat_contents[\"meta\"][0][0][3][0][0]):\n",
    "        for y in range(mat_contents[\"meta\"][0][0][4][0][0]):\n",
    "            for z in range(mat_contents[\"meta\"][0][0][5][0][0]):\n",
    "                col = mat_contents[\"meta\"][0][0][7][x][y][z]\n",
    "                matrix[:,col] = fmri[x-1][y-1][z-1][:]\n",
    "    matrices.append(matrix)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HKm-vOaOJwZH"
   },
   "outputs": [],
   "source": [
    "feature_matrix = np.zeros((5176,195)) #stores the feature vectors as a row for each word\n",
    "feature_names = [] #stores the names of all features in order\n",
    "feature_types = {} #stores the types of features and all the names of the features for each type\n",
    "\n",
    "features = sio.loadmat(fMRI_folder/'story_features.mat')\n",
    "feature_count = 0\n",
    "for feature_type in features['features'][0]:\n",
    "    feature_types[feature_type[0][0]] = []\n",
    "    if isinstance(feature_type[1][0], str):\n",
    "        feature_types[feature_type[0][0]].append(feature_type[1][0])\n",
    "        feature_names.append(feature_type[1][0])\n",
    "    else:\n",
    "        for feature in feature_type[1][0]:\n",
    "            feature_types[feature_type[0][0]].append(feature[0])\n",
    "            feature_names.append(feature[0])\n",
    "    feature_matrix[:, feature_count:feature_count+feature_type[2].shape[1]] = feature_type[2] #adds the (5176xN) feature values to the feature matrix for the current feature group\n",
    "    feature_count += feature_type[2].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9Ks5wTcKJmjz"
   },
   "outputs": [],
   "source": [
    "words_info = [] #stores tuples of (word, time, features) sorted by time appeared\n",
    "\n",
    "mat_file = 'subject_1.mat' #only looks at the first subject file, somewhere it said all the timings were the same so this should be safe\n",
    "mat_contents = sio.loadmat(fMRI_folder/mat_file)\n",
    "# print(mat_content)\n",
    "for count, row in enumerate(mat_contents['words'][0]):\n",
    "    word_value = row[0][0][0][0]\n",
    "    time = row[1][0][0]\n",
    "    \n",
    "    word_tuple = (word_value, time, feature_matrix[count,:])\n",
    "    words_info.append(word_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0DP3j_XQJnJR",
    "outputId": "094873e7-c700-4223-84ae-087150e3f246"
   },
   "outputs": [],
   "source": [
    "chapter_nine_text = \"\"\n",
    "for row in mat_contents['words'][0]:\n",
    "    chapter_nine_text += row[0][0][0][0] + \" \"\n",
    "# print(chapter_nine_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MjvOavtmUsTG",
    "outputId": "a69f8cd4-dd3c-47be-99a2-b1fcfdf391de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (53, 60, 50, 1311)\n",
      "1 (53, 60, 50, 1311)\n",
      "2 (53, 60, 50, 1311)\n",
      "3 (53, 60, 50, 1311)\n",
      "4 (53, 60, 50, 1311)\n",
      "5 (53, 60, 50, 1311)\n",
      "6 (53, 60, 50, 1311)\n",
      "7 (53, 60, 50, 1311)\n"
     ]
    }
   ],
   "source": [
    "for count, subject in enumerate(subjects_fmri):\n",
    "    print(count, np.shape(subject))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../fmri_indices', 'rb') as f:\n",
    "    fmri_indices = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "muTykRqfJcpY"
   },
   "source": [
    "Harrison's implementation of Gaussian weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "FOE-RCeGJcNV"
   },
   "outputs": [],
   "source": [
    "def hrf_alignment(num_words, words_info, NUM_SUBJS, subjects_fmri):\n",
    "\n",
    "    subjects_samples = [[] for i in range(NUM_SUBJS)] #stores lists of all the samples for each subject\n",
    "    window = signal.windows.gaussian(16, std=1) #gaussian window for the 4 fMRI scans\n",
    "    num_words = num_words\n",
    "    word_count = 0\n",
    "    while word_count < len(words_info) - 24:\n",
    "        #gets the 4 input words, and the 4 consecutive words while verifying they were read in sequence\n",
    "        scan_words = []\n",
    "        start_time = words_info[word_count][1]\n",
    "        in_sequence = True #tracks if the words are in sequence or not\n",
    "        for i in range(num_words):\n",
    "            word_info = words_info[word_count + i]\n",
    "            if word_info[1] != start_time + 0.5*i:\n",
    "                #if some of the words are not in sequence, skip forward 1 word after innter loop\n",
    "                in_sequence = False\n",
    "            scan_words.append(word_info[0])\n",
    "        if not in_sequence:\n",
    "            word_count +=1\n",
    "            continue\n",
    "        fmri_time = start_time #fMRI is taken at first word read\n",
    "        fmri_index = fmri_time//2 #since a scan happens every two seconds, the index is the time divided by 2\n",
    "        if not np.issubdtype(fmri_index, np.integer):\n",
    "            #if the first word is not aligned with the fmri scan (i.e. its not the first word in a TR)\n",
    "            word_count += 1\n",
    "            continue\n",
    "        break_found = False\n",
    "        for count, subject in enumerate(subjects_fmri):\n",
    "#             print(\"Subject:\",count)\n",
    "            #adds tuple of (fmri_scan, four words)\n",
    "            for i in range(num_words): #time delay of reading word after fMRI scan\n",
    "                delay = 0.5*i #delay after first word\n",
    "                for j in range(1,5): #get next 4 fMRI scans\n",
    "                    fMRI_delay = 2*j - delay #delay until jth next fMRI scan\n",
    "                    weight = window[int(2*fMRI_delay)-1] #gets the gaussian weighting (16 points instead of 8 to allow for .5)\n",
    "                    try:\n",
    "                        curr_fmri_idx = fmri_indices.index((start_time + math.floor(i/4) + 2*j)//2) #gets the index of the jth next fMRI scan\n",
    "                    except:\n",
    "#                         print(\"break found at index:\",(start_time + math.floor(i/4) + 2*j)//2)\n",
    "                        break_found = True\n",
    "                        break\n",
    "                    #adds weight fMRI scan\n",
    "                    if j == 1:\n",
    "                        word_scan = weight*subject[:][curr_fmri_idx]\n",
    "                    else:\n",
    "                        word_scan += weight*subject[:][curr_fmri_idx]\n",
    "                if break_found:\n",
    "                    break\n",
    "                if i == 0:\n",
    "                    summed_weighted_scans = word_scan\n",
    "                else:\n",
    "                    summed_weighted_scans += word_scan\n",
    "            if break_found:\n",
    "                break\n",
    "            subjects_samples[count].append((summed_weighted_scans/num_words, scan_words))\n",
    "        if break_found:\n",
    "            word_count += 1\n",
    "            continue \n",
    "#         print(\"Created sample:\")\n",
    "#         print(\"\\tScan time:\", str(start_time))\n",
    "#         print(\"\\tInput words:\", str(scan_words))\n",
    "        #if successful, skip forward to the next set of 4 words\n",
    "        word_count += 4\n",
    "    return subjects_samples\n",
    "\n",
    "    print(\"Total number of samples:\", str(len(subjects_samples[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "hDooPY-xnS96"
   },
   "outputs": [],
   "source": [
    "def get_fMRI_embedding(subjects_samples):\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    flattened_shape = reduce(np.multiply, subjects_samples[0][0].shape)\n",
    "    X_matrix = np.zeros((len(subjects_samples), flattened_shape))\n",
    "\n",
    "    for idx, sample in enumerate(subjects_samples):\n",
    "        tmp = sample[0]\n",
    "        # Reshape the voxels \n",
    "        tmp = tmp.ravel()\n",
    "        X_matrix[idx,:] = tmp\n",
    "        \n",
    "    # Apply voxelwise standardization        \n",
    "    X_matrix = scaler.fit_transform(X_matrix)\n",
    "    #print(np.shape(X_matrix))\n",
    "    return X_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'glove.840B.300d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kp/2c01k3yd3mxf7_5q6bcqn7rm0000gn/T/ipykernel_51498/2524037348.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mglove_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'glove.840B.300d.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mglove_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m         \"\"\"\n\u001b[0;32m-> 1719\u001b[0;31m         return _load_word2vec_format(\n\u001b[0m\u001b[1;32m   1720\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m             \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2047\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2048\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m             \u001b[0;31m# deduce both vocab_size & vector_size from 1st pass over file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mtransport_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     fobj = _shortcut_open(\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove.840B.300d.txt'"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import regex as re\n",
    "\n",
    "glove_file = 'glove.840B.300d.txt'\n",
    "glove_model = KeyedVectors.load_word2vec_format(glove_file, binary=False, no_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(word):\n",
    "    return re.sub(r'[^\\w\\s]', '', word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(word, model):\n",
    "    oov = np.zeros((300))\n",
    "    word = remove_punctuation(word)\n",
    "    if word == \"\":\n",
    "        word = \"unk\"\n",
    "    if word in model.index_to_key:\n",
    "        return model[word]\n",
    "    else:\n",
    "        return oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_embedding(subject_sample, model):\n",
    "\n",
    "    hits = 0\n",
    "    misses = 0\n",
    "    embeddings = []\n",
    "    embedding_matrix = np.zeros((len(subject_sample), 300))\n",
    "\n",
    "    for count, sentence in enumerate(subject_sample):\n",
    "        try:\n",
    "            sentence_embedding = [get_embedding(word, glove_model) for word in sentence[1]]\n",
    "            sentence_embedding = np.mean(sentence_embedding, axis=0)\n",
    "            embedding_matrix[count,:] = sentence_embedding\n",
    "        except ValueError:\n",
    "            print(sentence[1])\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ZoeD2JzgXvSY",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "2023-12-09 16:54:20.028702: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, DistilBertTokenizer,DistilBertModel\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#% matplotlib inline\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SBERT_embedding(subject_sample):\n",
    "    word_list = []\n",
    "    context_length = 0\n",
    "\n",
    "    for i in range(len(subject_sample)):\n",
    "        context_length = len(subject_sample[i][1])\n",
    "        word_list.append(subject_sample[i][1])\n",
    "    new_word_list = [(' '.join(sublist)) for sublist in word_list]\n",
    "    print(context_length)\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    x = model.encode(new_word_list,convert_to_tensor=True)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from absl import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "#module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "#model = hub.load(module_url)\n",
    "#print (\"module %s loaded\" % module_url)\n",
    "##def embed(input):\n",
    " # return model(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -r /var/folders/kp/2c01k3yd3mxf7_5q6bcqn7rm0000gn/T/tfhub_modules/744ecc8f0e54abfc94870bb7a86ec2c67e8724da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = \"https://www.kaggle.com/models/google/universal-sentence-encoder/frameworks/TensorFlow2/variations/universal-sentence-encoder/versions/2\"\n",
    "model_USE = hub.load(module_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = \" https://www.kaggle.com/models/google/universal-sentence-encoder/frameworks/TensorFlow2/variations/large/versions/2\"\n",
    "model_USE = hub.load(module_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_USE_embedding(subject_sample):\n",
    "    word_list = []\n",
    "    context_length = 0\n",
    "\n",
    "    for i in range(len(subject_sample)):\n",
    "        context_length = len(subject_sample[i][1])\n",
    "        word_list.append(subject_sample[i][1])\n",
    "    new_word_list = [(' '.join(sublist)) for sublist in word_list]\n",
    "    #print(new_word_list)\n",
    "    #print(context_length)\n",
    "    #model = hub.load(module_url)\n",
    "    #print (\"module %s loaded\" % module_url)\n",
    "    sentence_embeddings = model_USE(new_word_list)\n",
    "\n",
    "    return np.array(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_matrix = get_USE_embedding(subjects_samples[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jai's Implementation of clip embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_embedding(subject_sample, model,processor):\n",
    "    word_list = []\n",
    "    for i in range(len(subject_sample)):\n",
    "        word_list.append(subject_sample[i][1])\n",
    "    new_word_list = [' '.join(sublist) for sublist in word_list]\n",
    "\n",
    "    inputs = processor(text=new_word_list, images=image, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # text_embeds is in the same space as the given image\n",
    "        text_embeds = outputs['text_embeds']\n",
    "\n",
    "        # pooler_output is just the text embedding \n",
    "        # This is what we want to use I think \n",
    "        pooler_output = outputs['text_model_output']['pooler_output']\n",
    "#         print(pooler_output.numpy().shape)\n",
    "    #return pooler_output\n",
    "    return text_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "UzhyDXB3qNsU"
   },
   "outputs": [],
   "source": [
    "# split data into train and test\n",
    "\n",
    "def split_data(X_matrix, Y_matrix,random_seed):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_matrix, y_matrix,\n",
    "                                                        test_size=0.20,\n",
    "                                                        random_state=random_seed,\n",
    "                                                        shuffle=False)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "XTAuddpyeVcI"
   },
   "outputs": [],
   "source": [
    "# Train Ridge Regression Model\n",
    "\n",
    "def train_model(X_train, y_train):\n",
    "    \n",
    "    ridge_model = Ridge(alpha=1.0)\n",
    "    model = ridge_model.fit(X_train, y_train)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "D_M1TZx5Ru1n"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, random_text_embedding):\n",
    "    correct_predictions = 0\n",
    "\n",
    "    # Predict text embedding from test fMRI data\n",
    "    y_pred = model.predict(X_test)\n",
    "#     print(np.shape(y_pred))\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    prediction_similarity_true = cosine_similarity(y_test, y_pred)\n",
    "    prediction_similarity_random = cosine_similarity(random_text_embedding, y_pred)\n",
    "#     print(np.shape(prediction_similarity_true), np.shape(prediction_similarity_true))\n",
    "    \n",
    "     # Count the occurrences of \"1\" in the first column    \n",
    "    for i in range(X_test.shape[0]):\n",
    "        if prediction_similarity_true[i].mean() >= prediction_similarity_random.mean():\n",
    "            correct_predictions +=1\n",
    "        #print(correct_predicitions)\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    accuracy = (correct_predictions / X_test.shape[0])\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "Z4cuRbSqqRza",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def obtain_two_way_class_result(model, X_test, y_test, y_matrix, batches):\n",
    "    accuracy = []\n",
    "    for i in range(batches):\n",
    "        idx = np.arange(len(y_test))\n",
    "        np.random.shuffle(idx)\n",
    "        random_text_embedding = y_matrix[idx]\n",
    "        #print(random_text_embedding)\n",
    "        test_accuracy = evaluate_model(model, X_test[idx], y_test[idx], random_text_embedding)\n",
    "        accuracy.append(test_accuracy)\n",
    "\n",
    "    avg_accuracy = sum(accuracy)/len(accuracy)\n",
    "    \n",
    "    #print(accuracy)\n",
    "    #print(f\"Average Accuracy: {avg_accuracy * 100:.2f}%\")\n",
    "    \n",
    "    return avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from numpy import arange\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "from statistics import stdev, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seeds = [123,1234,122,1233,1235,1246,1232,1221,1542,1643,1231]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement leave_one_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fMRI_embedding_CV(subjects_samples,min_shape):\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    X_matrix = np.zeros((len(subjects_samples), min_shape))\n",
    "\n",
    "    for idx, sample in enumerate(subjects_samples):\n",
    "        tmp = sample[0][:min_shape]\n",
    "        # Reshape the voxels \n",
    "        tmp = tmp.ravel()\n",
    "        X_matrix[idx,:] = tmp\n",
    "        \n",
    "    # Apply voxelwise standardization        \n",
    "    X_matrix = scaler.fit_transform(X_matrix)\n",
    "    return X_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define min_voxel\n",
    "def get_min_voxel(matrices):\n",
    "    \n",
    "    size = []\n",
    "    for i in range(len(matrices)-1):\n",
    "        size.append(len(matrices[i][1][0]))\n",
    "    min_size = min(size)\n",
    "    print(min_size)\n",
    "    return min_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train_test_split\n",
    "\n",
    "def get_train_test_split(subjects_samples, text_encoding_name, train_label):\n",
    "    \n",
    "    # get minimum voxel value\n",
    "    min_size = get_min_voxel(subjects_samples)\n",
    "    samples = []\n",
    "    all_of = []\n",
    "    ytrain = [train_label for x in range(7)]\n",
    "    y_train = np.concatenate(ytrain, axis=0) \n",
    "\n",
    "    for i in range(len(subjects_samples)):\n",
    "        train_data = get_fMRI_embedding_CV(subjects_samples[i],min_size)\n",
    "        samples.append(train_data)\n",
    "\n",
    "    \n",
    "    for x in range(8):\n",
    "        train = [samples[a] for a in range(8) if a != x]  \n",
    "        X_train = np.concatenate(train, axis=0) \n",
    "        #X_test = get_fMRI_embedding_CV(subjects_samples[x],min_size)\n",
    "        all_of.append([X_train, y_train, samples[x], train_label])\n",
    "    \n",
    "    return all_of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context length: 8---------------------------------------------\n",
      "62598\n",
      "Average Accuracy: 60.26%\n",
      "Average Accuracy: 57.66%\n",
      "Average Accuracy: 60.83%\n",
      "Average Accuracy: 61.15%\n",
      "Average Accuracy: 59.69%\n",
      "Average Accuracy: 60.58%\n",
      "Average Accuracy: 60.83%\n"
     ]
    }
   ],
   "source": [
    "context_lengths = [8,12,16]\n",
    "NUM_SUBJS = 8\n",
    "text_encoding_name = \"SBERT\"\n",
    "for length in context_lengths:\n",
    "    # Step 1: Obtain aligned fMRI embedding with words\n",
    "    print(f'Context length: {length}---------------------------------------------')\n",
    "    subjects_samples = hrf_alignment(length, words_info, NUM_SUBJS, matrices)\n",
    "#     X_train, y_train, X_test, y_test = get_train_test_split(subjects_samples, 'glove', glove_model)\n",
    "    avg = []\n",
    "    if text_encoding_name == 'glove':\n",
    "        train_label = get_glove_embedding(subjects_samples[0],model)\n",
    "        #ytrain.append(train_label)\n",
    "    elif text_encoding_name == 'clip':\n",
    "        train_label = get_clip_embedding(subjects_samples[0],clip_model,clip_processor)\n",
    "        #ytrain.append(train_label)\n",
    "    elif text_encoding_name == 'USE':\n",
    "        train_label = get_USE_embedding(subjects_samples[0])\n",
    "        #ytrain.append(train_label)\n",
    "    elif text_encoding_name == 'SBERT':\n",
    "        train_label = get_SBERT_embedding(subjects_samples[0])\n",
    "    \n",
    "    list_all = get_train_test_split(subjects_samples, text_encoding_name,train_label)\n",
    "\n",
    "    for x in range(8):\n",
    "        #X_train, y_train, X_test, y_test = get_train_test_split(subjects_samples, text_encoding_name, x,train_label)\n",
    "    #     X_train, y_train, X_test, y_test = get_train_test_split(subjects_samples, 'SBERT')\n",
    "    #     X_train, y_train, X_test, y_test = get_train_test_split(subjects_samples, 'USE')\n",
    "\n",
    "        \n",
    "        # Train model\n",
    "        model = train_model(list_all[x][0], list_all[x][1])\n",
    "\n",
    "        # Evaluate model\n",
    "        batches = 8\n",
    "        #if x == 7:\n",
    "        accuracy = obtain_two_way_class_result(model, list_all[x][2], list_all[x][3], list_all[0][3], batches)\n",
    "        #else:\n",
    "            #accuracy = obtain_two_way_class_result(model, list_all[x+1][2], list_all[x+1][3], list_all[x+1][3], batches)\n",
    "\n",
    "        print(f'Average Accuracy: {accuracy * 100:.2f}%')\n",
    "        avg.append(accuracy * 100)\n",
    "    print(mean(avg),stdev(avg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kp/2c01k3yd3mxf7_5q6bcqn7rm0000gn/T/ipykernel_70556/4213807050.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m rects3 = ax.bar(ind, [x-offset for x in USE_T_means], width, yerr=USE_T_std,\n\u001b[1;32m     21\u001b[0m                 label='USE-T', error_kw=dict(lw=3, capsize=5, capthick=3),bottom=offset)\n\u001b[0;32m---> 22\u001b[0;31m rects4 = ax.bar(ind + width, [x-offset for x in SBERT_means], width, yerr=SBERT_std,\n\u001b[0m\u001b[1;32m     23\u001b[0m                 label='SBERT', error_kw=dict(lw=3, capsize=5, capthick=3),bottom=offset)\n\u001b[1;32m     24\u001b[0m rects5 = ax.bar(ind + width*2, [x-offset for x in CLIP_means], width, yerr=CLIP_std,\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "CLIP_means, CLIP_std = (55.31, 59.67, 63.27, 63.93, 63.23), (2.92, 6.28, 6.00, 9.19, 6.39)\n",
    "USE_DAN_means, USE_DAN_std = (50.36, 54.86, 50.84, 59.78, 55.24), (5.06, 5.81, 8.43, 9.1, 8.3)\n",
    "USE_T_means, USE_T_std = (53.81, 55.34 , 59.94, 68.30, 61.10), (6.01, 8.16, 8.79, 10.59, 8.79)\n",
    "SBERT_means, SBERT_std = (54.59), (0.86)\n",
    "GloVe_means, GloVe_std = (52.55, 52.71, 51.87, 50.22 , 54.06), (5.82, 6.16, 6.20, 4.11, 6.99)\n",
    "offset = 40\n",
    "ind = np.arange(len(USE_T_means))  # the x locations for the groups\n",
    "width = 0.18  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(ind - width*2, [x-offset for x in GloVe_means], width, yerr=GloVe_std,\n",
    "                label='GloVe', error_kw=dict(lw=3, capsize=5, capthick=3),bottom=offset)\n",
    "rects2 = ax.bar(ind- width, [x-offset for x in USE_DAN_means], width, yerr=USE_DAN_std,\n",
    "                label='USE-DAN', error_kw=dict(lw=3, capsize=5, capthick=3),bottom=offset)\n",
    "rects3 = ax.bar(ind, [x-offset for x in USE_T_means], width, yerr=USE_T_std,\n",
    "                label='USE-T', error_kw=dict(lw=3, capsize=5, capthick=3),bottom=offset)\n",
    "rects4 = ax.bar(ind + width, [x-offset for x in SBERT_means], width, yerr=SBERT_std,\n",
    "                label='SBERT', error_kw=dict(lw=3, capsize=5, capthick=3),bottom=offset)\n",
    "rects5 = ax.bar(ind + width*2, [x-offset for x in CLIP_means], width, yerr=CLIP_std,\n",
    "                label='CLIP', error_kw=dict(lw=3, capsize=5, capthick=3),bottom=offset)\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Accuracy', weight='bold')\n",
    "ax.set_title('1vs2 Accuracy of Different Text Encoders', weight='bold')\n",
    "ax.set_xticks(ind)\n",
    "ax.xaxis.set_tick_params(width=5,length=15)\n",
    "ax.yaxis.set_tick_params(width=5,length=15)\n",
    "ax.set_xticklabels(( '8 words', '12 words', '16 words'), weight='bold')\n",
    "ax.legend()\n",
    "plt.rcParams['figure.figsize'] = [30,20]\n",
    "font = {'family': [\"normal\"],\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 40}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "#plt.yticks( size=50) \n",
    "#plt.rc('ytick', labelsize=50) \n",
    "#fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
